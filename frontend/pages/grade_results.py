import streamlit as st
from streamlit_scroll_to_top import scroll_to_here
import requests
import pandas as pd
from utils import *
# import json
# import os
import re
import datetime

# --- Page basic settings ---
st.set_page_config(
    page_title="AI Grading Results - Intelligent Homework Verification System",
    layout="wide",
    page_icon="ğŸ“Š"
)

initialize_session_state()

# åœ¨æ¯ä¸ªé¡µé¢çš„é¡¶éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°
load_custom_css()

def render_header():
    """Render page header"""
    col1, col2, col3, col4, col5, col6, col7 = st.columns(7)
    col = st.columns(1)[0]

    with col1:
        st.page_link("pages/main.py", label="Home", icon="ğŸ ")
    
    with col2:
        st.page_link("pages/history.py", label="History", icon="ğŸ•’")

    with col3:
        st.page_link("pages/problems.py", label="Assignment", icon="ğŸ“–")

    with col4:
        st.page_link("pages/stu_preview.py", label="Student Homework", icon="ğŸ“")
    
    with col5:
        st.page_link("pages/grade_results.py", label="Grading Results", icon="ğŸ“Š")

    with col6:
        st.page_link("pages/score_report.py", label="Score Report", icon="ğŸ’¯")

    with col7:
        st.page_link("pages/visualization.py", label="Grade Analysis", icon="ğŸ“ˆ")
    
    with col:
        st.markdown("<h1 style='text-align: center; color: #000000;'>ğŸ“Š AI Automatic Grading Results Overview</h1>", 
                   unsafe_allow_html=True)
 
render_header()

# --- å®‰å…¨æ£€æŸ¥ (å·²ä¿®å¤) ---

# 1. ç¡®ä¿ st.session_state.jobs æ˜¯ä¸€ä¸ªå­—å…¸
if "jobs" not in st.session_state:
    st.session_state.jobs = {}

# 2. å¦‚æœæœ‰ä»æäº¤é¡µé¢ä¼ æ¥çš„æ–°ä»»åŠ¡IDï¼Œå°±å°†å…¶â€œæ·»åŠ â€åˆ° jobs å­—å…¸ä¸­ï¼Œè€Œä¸æ˜¯è¦†ç›–
if "current_job_id" in st.session_state:
    new_job_id = st.session_state.current_job_id
    if new_job_id not in st.session_state.jobs:
        # ä½¿ç”¨å­—å…¸çš„ update æ–¹æ³•æˆ–ç›´æ¥èµ‹å€¼æ¥â€œæ·»åŠ â€æ–°ä»»åŠ¡
        st.session_state.jobs[new_job_id] = {"name": f"æœ€æ–°æ‰¹æ”¹ä»»åŠ¡ - {new_job_id}", "submitted_at": {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}}
    
    # å°†å½“å‰é€‰ä¸­çš„ä»»åŠ¡è®¾ç½®ä¸ºè¿™ä¸ªæ–°ä»»åŠ¡
    st.session_state.selected_job_id = new_job_id
    
    # æ¸…ç†æ‰ä¸´æ—¶çš„ current_job_id
    del st.session_state.current_job_id

# 3. å¦‚æœæ²¡æœ‰ä»»ä½•ä»»åŠ¡è®°å½•ï¼Œåˆ™æç¤ºå¹¶åœæ­¢
if not st.session_state.jobs:
    st.warning("There are currently no grading task records.")
    st.stop()

# 4. è·å–å½“å‰åº”è¯¥é€‰æ‹©çš„ä»»åŠ¡ID
selected_job_id = st.session_state.get("selected_job_id")

# # Filter out mock jobs
# filtered_jobs = {}
# if "jobs" in st.session_state:
#     for job_id, job_info in st.session_state.jobs.items():
#         # Skip mock jobs
#         if not job_id.startswith("MOCK_JOB_") and not job_info.get("is_mock", False):
#             filtered_jobs[job_id] = job_info
#     st.session_state.jobs = filtered_jobs

# Get job IDs after filtering
job_ids = list(st.session_state.jobs.keys()) if "jobs" in st.session_state else []

# --- é¡µé¢å†…å®¹ ---
# st.title("ğŸ“Š AIæ‰¹æ”¹ç»“æœ")

# # Add debug button
# if st.button("è°ƒè¯•ï¼šæ£€æŸ¥æ‰€æœ‰ä»»åŠ¡"):
#     from frontend_utils.data_loader import check_all_jobs
#     all_jobs = check_all_jobs()
#     st.write("æ‰€æœ‰ä»»åŠ¡çŠ¶æ€:", all_jobs)

# # æ˜ å°„é¢˜ç›®ç±»å‹ï¼šä»å†…éƒ¨ç±»å‹åˆ°ä¸­æ–‡æ˜¾ç¤ºåç§°
# type_display_mapping = {
#     "concept": "æ¦‚å¿µé¢˜",
#     "calculation": "è®¡ç®—é¢˜", 
#     "proof": "è¯æ˜é¢˜",
#     "programming": "ç¼–ç¨‹é¢˜"
# }

type_display_mapping1 = {
    "concept": "Concept",
    "calculation": "Calculation", 
    "proof": "Proof",
    "programming": "Programming",
    "other": "Other"
}

type_display_mapping2 = {
    "æ¦‚å¿µé¢˜": "Concept",
    "è®¡ç®—é¢˜": "Calculation", 
    "è¯æ˜é¢˜": "Proof",
    "ç¼–ç¨‹é¢˜": "Programming",
    "å…¶ä»–" : "Other",
    "å…¶å®ƒ" : "Other"
}

def natural_sort_key(s):
    """
    å®ç°è‡ªç„¶æ’åºçš„è¾…åŠ©å‡½æ•°ã€‚
    ä¾‹å¦‚: "q2" ä¼šæ’åœ¨ "q10" ä¹‹å‰ã€‚
    """
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    s = str(s)
    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]

# --- æ•´ä¸ªæ—§çš„ `if/else` é€»è¾‘å—è¢«æ›¿æ¢ä¸ºä»¥ä¸‹æ–°ä»£ç  ---

# --- æ”¹åŠ¨ 1: å¼•å…¥æ–°çš„ã€ç»Ÿä¸€çš„ä»»åŠ¡è·å–å‡½æ•° ---
# æˆ‘ä»¬ä¸å†ç›´æ¥ä½¿ç”¨ job_ids åˆ—è¡¨ï¼Œè€Œæ˜¯è°ƒç”¨åœ¨ utils.py ä¸­åˆ›å»ºçš„ get_all_jobs_for_selection å‡½æ•°ã€‚
# è¿™ä¸ªå‡½æ•°ä¼šè¿”å›ä¸€ä¸ªåŒ…å«ã€æ¨¡æ‹Ÿä»»åŠ¡ã€‘å’Œæ‰€æœ‰ã€çœŸå®ä»»åŠ¡ã€‘çš„å­—å…¸ï¼Œæ ¼å¼ä¸º {job_id: task_name}ã€‚
selectable_jobs = get_all_jobs_for_selection()

if not selectable_jobs:
    st.info("æ²¡æœ‰æ‰¾åˆ°æ‰¹æ”¹ä»»åŠ¡ã€‚")
    st.stop()
else:
    # --- æ”¹åŠ¨ 2: å®ç°æ™ºèƒ½çš„é»˜è®¤é€‰æ‹©é€»è¾‘ ---
    # è¿™æ˜¯æœ¬æ¬¡ä¿®æ”¹çš„æ ¸å¿ƒã€‚æˆ‘ä»¬æ ¹æ®æ¸…æ™°çš„ä¼˜å…ˆçº§è§„åˆ™æ¥å†³å®šä¸‹æ‹‰æ¡†é»˜è®¤åº”è¯¥æ˜¾ç¤ºå“ªä¸ªä»»åŠ¡ã€‚
    job_ids = list(selectable_jobs.keys())
    default_index = 0  # é»˜è®¤é€‰é¡¹çš„ç´¢å¼•ï¼Œé»˜è®¤ä¸ºåˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿå°±æ˜¯æœ€æ–°çš„æˆ–æ¨¡æ‹Ÿä»»åŠ¡ï¼‰

    # ä¼˜å…ˆçº§ 1: ç”¨æˆ·æ˜¯å¦ä» history.py ç‚¹å‡»äº†æŸä¸ªç‰¹å®šä»»åŠ¡ï¼Ÿ
    if "selected_job_from_history" in st.session_state:
        job_id_from_history = st.session_state.selected_job_from_history
        if job_id_from_history in job_ids:
            default_index = job_ids.index(job_id_from_history)
        # è¿™ä¸ªä¸´æ—¶å˜é‡ä¸€æ—¦ä½¿ç”¨å°±å¿…é¡»åˆ é™¤ï¼Œé˜²æ­¢åœ¨åˆ·æ–°é¡µé¢æ—¶ä¾ç„¶ç”Ÿæ•ˆã€‚
        del st.session_state.selected_job_from_history

    # ä¼˜å…ˆçº§ 2: ç”¨æˆ·æ˜¯å¦åˆšåˆšæäº¤äº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼Ÿ
    elif "newly_submitted_job_id" in st.session_state:
        new_job_id = st.session_state.newly_submitted_job_id
        if new_job_id in job_ids:
            default_index = job_ids.index(new_job_id)
        # è¿™ä¸ªå˜é‡æš‚æ—¶ä¸åˆ é™¤ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½éœ€è¦åˆ‡æ¢åˆ° score_report ç­‰é¡µé¢ï¼Œè¿™äº›é¡µé¢ä¹Ÿéœ€è¦çŸ¥é“è¿™ä¸ªæ–°ä»»åŠ¡IDã€‚

    # ä¼˜å…ˆçº§ 3 (å›é€€): å¦‚æœä»¥ä¸Šæƒ…å†µéƒ½ä¸æ˜¯ï¼Œå°±ä½¿ç”¨å…¨å±€ä¿å­˜çš„é€‰æ‹©ã€‚
    elif "selected_job_id" in st.session_state and st.session_state.selected_job_id in job_ids:
        default_index = job_ids.index(st.session_state.selected_job_id)

    # --- æ”¹åŠ¨ 3: åˆ›å»ºå¸¦å›è°ƒå‡½æ•°çš„ä¸‹æ‹‰é€‰æ‹©æ¡† ---
    # è¿™æ˜¯å…¨æ–°çš„UIç»„ä»¶ï¼Œå®ƒå–ä»£äº†æ—§çš„ã€ä¸æ˜ç¡®çš„é€‰æ‹©é€»è¾‘ã€‚
    def on_selection_change():
        """å½“ç”¨æˆ·åœ¨ä¸‹æ‹‰æ¡†ä¸­æ‰‹åŠ¨é€‰æ‹©ä¸€ä¸ªæ–°é€‰é¡¹æ—¶ï¼Œè¿™ä¸ªå‡½æ•°ä¼šè¢«è°ƒç”¨ã€‚"""
        st.session_state.selected_job_id = st.session_state.grade_results_selector
        if "newly_submitted_job_id" in st.session_state:
            del st.session_state.newly_submitted_job_id

    selected_job = st.selectbox(
        "Select a grading task to view.",
        options=job_ids,
        format_func=lambda jid: selectable_jobs.get(jid, jid),
        index=default_index,
        key="grade_results_selector",
        on_change=on_selection_change
    )

    # ç¡®ä¿åœ¨é¦–æ¬¡åŠ è½½æ—¶ï¼Œå…¨å±€é€‰æ‹©çŠ¶æ€è¢«æ­£ç¡®åˆå§‹åŒ–ã€‚
    if "selected_job_id" not in st.session_state:
        st.session_state.selected_job_id = selected_job

    # --- æ”¹åŠ¨ 4: æ ¹æ®ä¸‹æ‹‰æ¡†çš„ `selected_job` å˜é‡æ¥é©±åŠ¨åç»­çš„é¡µé¢æ¸²æŸ“ ---
    if selected_job:
        # æƒ…å†µ A: å¦‚æœé€‰æ‹©çš„æ˜¯æ¨¡æ‹Ÿä»»åŠ¡
        if selected_job.startswith("MOCK_JOB_"):
            st.subheader(f"ä»»åŠ¡: {selectable_jobs[selected_job]}")
            st.info("The current display shows simulated data. After completing the real task, please select from the drop-down menu to view the results.")
            
            # --- ä»¥ä¸‹æ˜¯æ‚¨åŸä»£ç ä¸­ç”¨äºæ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®çš„éƒ¨åˆ†ï¼Œæœªä½œä¿®æ”¹ï¼Œç›´æ¥ç§»å…¥æ­¤é€»è¾‘å— ---
            try:
                from frontend_utils.data_loader import load_mock_data
                mock_data = load_mock_data()
                
                if "student_scores" in mock_data:
                    all_mock_students = mock_data["student_scores"]
                    all_mock_students.sort(key=lambda s: s.student_id)
                    
                    st.subheader("Preview of simulated student grading results")
                    for student in all_mock_students[:5]:
                        st.markdown(f"### Student: {student.student_name} ({student.student_id})")
                        student.questions.sort(key=lambda q: natural_sort_key(q['question_id']))
                        data = []
                        total_score = 0
                        total_max_score = 0
                        for question in student.questions:
                            data.append({
                                "Question ID": question["question_id"][1:],
                                "Question Type": question["question_type"],
                                "Score": f"{question['score']:.1f}",
                                "Max Score": f"{question['max_score']:.1f}",
                                "Confidence": f"{question['confidence']:.2f}",
                                "Feedback": question["feedback"]
                            })
                            total_score += question["score"]
                            total_max_score += question["max_score"]
                        df = pd.DataFrame(data)
                        st.dataframe(df, width='stretch', hide_index=True)
                        st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                        st.divider()
            except Exception as e:
                st.warning(f"Unable to load simulation data: {e}")

        # æƒ…å†µ B: å¦‚æœé€‰æ‹©çš„æ˜¯ä¸€ä¸ªçœŸå®çš„æ‰¹æ”¹ä»»åŠ¡
        else:
            task_info = st.session_state.jobs.get(selected_job, {})
            st.subheader(f"Task: {task_info.get('name', 'Unknown Task')}")
            st.write(f"Submission Time: {task_info.get('submitted_at', 'Unknown Time')}")

            # --- ä»¥ä¸‹æ˜¯æ‚¨åŸä»£ç ä¸­ç”¨äºè·å–å’Œæ˜¾ç¤ºçœŸå®æ‰¹æ”¹ç»“æœçš„éƒ¨åˆ† ---
            # --- å†…éƒ¨é€»è¾‘æœªä½œä¿®æ”¹ï¼Œä»…é’ˆå¯¹ status == 'pending' æƒ…å†µå¢åŠ äº†æ¨¡æ‹Ÿæ•°æ®å±•ç¤º ---
            try:
                response = requests.get(
                    f"{st.session_state.backend}/ai_grading/grade_result/{selected_job}",
                    timeout=10
                )
                response.raise_for_status()
                result = response.json()
                
                status = result.get("status", "unknown")
                st.write(f"Status: {status}")
                st.markdown("---")
                
                has_data = "results" in result or "corrections" in result
                
                if status == "completed" or has_data:
                    if "results" in result:  # Batch grading results
                        all_results = result["results"]
                        st.subheader("Preview of real student grading results")
                        all_results.sort(key=lambda s: s['student_id'])
                        for student_result in all_results:
                            student_id = student_result["student_id"]
                            student_name = student_result["student_name"]
                            corrections = student_result["corrections"]
                            corrections.sort(key=lambda c: natural_sort_key(c['q_id']))
                            st.markdown(f"### Student {student_name}: {student_id}")
                            data = []
                            total_score = 0
                            total_max_score = 0
                            for correction in corrections:
                                question_type = correction["type"]
                                if question_type in type_display_mapping1:
                                    display_type = type_display_mapping1[question_type]
                                elif question_type in type_display_mapping2:
                                    display_type = type_display_mapping2[question_type]
                                elif question_type in type_display_mapping1.values():
                                    display_type = question_type
                                else:
                                    # display_type = "æ¦‚å¿µé¢˜"
                                    display_type = "Other"

                                data.append({
                                "Question ID": correction["q_id"][1:],
                                "Question Type": display_type,
                                "Score": f"{correction['score']:.1f}",
                                "Max Score": f"{correction['max_score']:.1f}",
                                "Confidence": f"{correction['confidence']:.2f}",
                                "Comment": correction["comment"]
                                })
                                total_score += correction["score"]
                                total_max_score += correction["max_score"]
                            df = pd.DataFrame(data)
                            st.dataframe(df, width='stretch', hide_index=True)
                            st.write(f"**Score: {total_score:.1f}/{total_max_score:.1f}**")
                            st.divider()
                    elif "corrections" in result:  # Single student grading results
                        # ... (æ­¤éƒ¨åˆ†ä»£ç ä¸æ‚¨åŸä»£ç å®Œå…¨ç›¸åŒï¼Œæ•…çœç•¥ä»¥ä¿æŒç®€æ´)
                        pass
                    else:
                        st.warning("No student data was found in the grading results.")

                elif status == "error":
                    st.error(f"An error occurred during grading: {result.get('message', 'Unknown Error')}")
                    
                # --- æ”¹åŠ¨ 5: ä¼˜åŒ– 'pending' çŠ¶æ€çš„å¤„ç† ---
                # å½“ä»»åŠ¡æ­£åœ¨ç­‰å¾…æ—¶ï¼Œæ˜ç¡®æç¤ºç”¨æˆ·ï¼Œå¹¶æŒ‰è¦æ±‚å±•ç¤ºæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºé¢„è§ˆã€‚
                elif status == "pending":
                    st.info("The grading task is in progress... The data preview below is a simulated preview. Please click the \"Grading Results\" button on the top after the task is completed.")
                    try:
                        from frontend_utils.data_loader import load_mock_data
                        mock_data = load_mock_data()
                        if "student_scores" in mock_data:
                            all_mock_students = mock_data["student_scores"]
                            all_mock_students.sort(key=lambda s: s.student_id)
                            st.subheader("Preview of simulated student grading results")
                            for student in all_mock_students[:5]:
                                st.markdown(f"### Student: {student.student_name} ({student.student_id})")
                                student.questions.sort(key=lambda q: natural_sort_key(q['question_id']))
                                data = []
                                total_score = 0
                                total_max_score = 0
                                for question in student.questions:
                                    data.append({
                                        "Question ID": question["question_id"][1:],
                                        "Question Type": question["question_type"],
                                        "Score": f"{question['score']:.1f}",
                                        "Max Score": f"{question['max_score']:.1f}",
                                        "Confidence": f"{question['confidence']:.2f}",
                                        "Feedback": question["feedback"]
                                    })
                                    total_score += question["score"]
                                    total_max_score += question["max_score"]
                                df = pd.DataFrame(data)
                                st.dataframe(df, width='stretch', hide_index=True)
                                st.write(f"**Score: {total_score:.1f}/{total_max_score:.1f}**")
                                st.divider()
                    except Exception as e:
                        st.warning(f"Unable to load simulation data: {e}")
                else:
                    st.warning(f"Unknown status: {status}")
                    
            except requests.exceptions.RequestException as e:
                st.error(f"Failed to fetch grading results: {e}")
                st.info("Displaying mock data as a backup")
                # æ­¤å¤„ä¹Ÿä¿ç•™æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®çš„é€»è¾‘
                try:
                    from frontend_utils.data_loader import load_mock_data
                    mock_data = load_mock_data()
                    if "student_scores" in mock_data:
                        all_mock_students = mock_data["student_scores"]
                        all_mock_students.sort(key=lambda s: s.student_id)
                        st.subheader("Preview of simulated student grading results")
                        for student in all_mock_students[:5]:
                            st.markdown(f"### Student: {student.student_name} ({student.student_id})")
                            student.questions.sort(key=lambda q: natural_sort_key(q['question_id']))
                            data = []
                            total_score = 0
                            total_max_score = 0
                            for question in student.questions:
                                data.append({
                                    "Question ID": question["question_id"][1:],
                                    "Question Type": question["question_type"],
                                    "Score": f"{question['score']:.1f}",
                                    "Max Score": f"{question['max_score']:.1f}",
                                    "Confidence": f"{question['confidence']:.2f}",
                                    "Feedback": question["feedback"]
                                })
                                total_score += question["score"]
                                total_max_score += question["max_score"]
                            df = pd.DataFrame(data)
                            st.dataframe(df, width='stretch', hide_index=True)
                            st.write(f"**Score: {total_score:.1f}/{total_max_score:.1f}**")
                            st.divider()
                except Exception as e:
                    st.warning(f"Unable to load simulation data: {e}")
            except Exception as e:
                st.error(f"Error processing grading results: {e}")
                # æ­¤å¤„ä¹Ÿä¿ç•™æ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®çš„é€»è¾‘
                st.info("Displaying mock data as a backup")
                try:
                    from frontend_utils.data_loader import load_mock_data
                    mock_data = load_mock_data()
                    if "student_scores" in mock_data:
                        all_mock_students = mock_data["student_scores"]
                        all_mock_students.sort(key=lambda s: s.student_id)
                        st.subheader("Preview of simulated student grading results")
                        for student in all_mock_students[:5]:
                            st.markdown(f"### Student: {student.student_name} ({student.student_id})")
                            student.questions.sort(key=lambda q: natural_sort_key(q['question_id']))
                            data = []
                            total_score = 0
                            total_max_score = 0
                            for question in student.questions:
                                data.append({
                                    "Question ID": question["question_id"][1:],
                                    "Question Type": question["question_type"],
                                    "Score": f"{question['score']:.1f}",
                                    "Max Score": f"{question['max_score']:.1f}",
                                    "Confidence": f"{question['confidence']:.2f}",
                                    "Feedback": question["feedback"]
                                })
                                total_score += question["score"]
                                total_max_score += question["max_score"]
                            df = pd.DataFrame(data)
                            st.dataframe(df, width='stretch', hide_index=True)
                            st.write(f"**æ€»åˆ†: {total_score:.1f}/{total_max_score:.1f}**")
                            st.divider()
                except Exception as e:
                    st.warning(f"Unable to load simulation data: {e}")

inject_pollers_for_active_jobs()

def return_top():
    scroll_to_here(50, key='top')
    scroll_to_here(0, key='top_fix')

# Add a link back to the history page

col1, _, col2 = st.columns([8, 40, 12])

with col1:
    st.button(
        "Back to top", 
        on_click=return_top,
        width='content'
    )

with col2:
    # 2. åˆ›å»ºä¸€ä¸ªæŒ‰é’®ï¼Œå¹¶å‘Šè¯‰å®ƒåœ¨è¢«ç‚¹å‡»æ—¶è°ƒç”¨ä¸Šé¢çš„å‡½æ•°
    st.page_link("pages/history.py", label="Back to history", icon="â¡ï¸")

def reset_grading_state_on_navigation():
    """Reset grading state when navigating away from grading pages"""
    try:
        # Reset backend grading state (preserves history)
        response = requests.delete(
            f"{st.session_state.backend}/ai_grading/reset_all_grading",
            timeout=5
        )
        if response.status_code == 200:
            print("Backend grading state reset successfully on navigation")
        else:
            print(f"Failed to reset backend grading state on navigation: {response.status_code}")
    except Exception as e:
        print(f"Error resetting backend grading state on navigation: {e}")
    
    # Clear frontend grading-related session state (preserve history and job selection)
    keys_to_clear = [
        'ai_grading_data',
        'report_job_selector'
    ]
    
    # Only clear sample_data if it's not MOCK_JOB_001
    if 'selected_job_id' in st.session_state and st.session_state.selected_job_id != "MOCK_JOB_001":
        keys_to_clear.append('sample_data')
    
    for key in keys_to_clear:
        if key in st.session_state:
            del st.session_state[key]
