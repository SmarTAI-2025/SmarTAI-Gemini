"""
Calculation question correction node implementation.
"""
import structlog
import re
import os
import json
import argparse
from typing import Dict, Any, List
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool

from backend.models import Correction, StepScore
from backend.correct.prompt_utils import prepare_calc_prompt
from backend.dependencies import get_llm, OPENAI_API_KEY, OPENAI_API_BASE, OPENAI_MODEL

# Setup logger
logger = structlog.get_logger()

# Zhipu AI configuration is now imported from dependencies.py

# Global LLM client for connection pooling
LLM_CLIENT = None

def get_llm_client():
    """Get or create a shared LLM client for connection pooling."""
    global LLM_CLIENT
    if LLM_CLIENT is None:
        LLM_CLIENT = get_llm()
    return LLM_CLIENT

class AnswerUnit(BaseModel):
    """Model for calculation answer unit."""
    q_id: str
    stem: str
    text: str
    correct_ans: str
    steps: List[Dict[str, Any]]

def parse_llm_json_response(response_text: str) -> Dict[str, Any]:
    """
    Parse LLM JSON response, handling common formatting issues.
    
    Args:
        response_text: The raw response text from the LLM
        
    Returns:
        Dict[str, Any]: Parsed JSON object
    """
    # Log the raw response for debugging
    logger.info("parsing_llm_response", response_text=response_text[:500] + "..." if len(response_text) > 500 else response_text)
    
    # Use a more robust approach similar to what's in routers/new.py
    # First try to find JSON objects in the response
    import json
    from pydantic import ValidationError
    
    # Match JSON objects or arrays
    match = re.search(r'\{.*\}', response_text, re.DOTALL)
    if not match:
        match = re.search(r'\[.*\]', response_text, re.DOTALL)
    
    if match:
        json_str = match.group(0)
        try:
            # Try to parse the JSON directly
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # If direct parsing fails, try to fix common issues
            try:
                # Remove trailing commas before closing braces/brackets
                json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
                # Remove comments
                json_str = re.sub(r'//.*?(?=\n|$)', '', json_str)
                json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
                # Try parsing again
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
    
    # If all parsing attempts fail, create a fallback response
    # Extract individual fields with more robust regex patterns
    try:
        llm_response = {}
        
        # Extract score with more flexible pattern matching
        score_match = re.search(r'"score"\s*:\s*([0-9.]+)', response_text)
        if score_match:
            llm_response["score"] = float(score_match.group(1))
        
        # Extract max_score
        max_score_match = re.search(r'"max_score"\s*:\s*([0-9.]+)', response_text)
        if max_score_match:
            llm_response["max_score"] = float(max_score_match.group(1))
        else:
            llm_response["max_score"] = 10.0
        
        # Extract confidence
        confidence_match = re.search(r'"confidence"\s*:\s*([0-9.]+)', response_text)
        if confidence_match:
            llm_response["confidence"] = float(confidence_match.group(1))
        else:
            llm_response["confidence"] = 0.8
        
        # Extract comment (handle escaped quotes and truncated text)
        comment_match = re.search(r'"comment"\s*:\s*"((?:[^"\\]|\\.)*)"', response_text)
        if comment_match:
            # Unescape the string
            comment = comment_match.group(1).replace('\\"', '"').replace('\\\\', '\\')
            llm_response["comment"] = comment
        else:
            # Try to extract comment with more flexible pattern
            flexible_comment_match = re.search(r'"comment"\s*:\s*"([^"]*)"', response_text)
            if flexible_comment_match:
                llm_response["comment"] = flexible_comment_match.group(1)
            else:
                llm_response["comment"] = "AI grading completed!"
        
        # Initialize empty steps array
        llm_response["steps"] = []
        
        logger.info("manual_json_parsing_success", parsed_keys=list(llm_response.keys()))
        return llm_response
    except Exception as e:
        logger.warning("manual_json_parsing_failed", error=str(e))
    
    # Final fallback
    llm_response = {
        "score": 5.0,
        "max_score": 10.0,
        "confidence": 0.8,
        "comment": "Default scoring",
        "steps": []
    }
    
    logger.info("parse_llm_json_response_complete", response_type=type(llm_response).__name__, 
                response_keys=list(llm_response.keys()) if isinstance(llm_response, dict) else "Not a dict")
    return llm_response

async def calc_node(answer_unit: Dict[str, Any], rubric: str, max_score: float = 10.0, llm=None) -> Correction:
    """
    Calculation question correction node.
    
    Args:
        answer_unit: The answer unit containing the student's answer steps
        rubric: The grading rubric
        max_score: The maximum score for this question
        llm: Optional LLM client to use for processing (if None, uses shared client)
        
    Returns:
        Correction: The correction result
    """
    logger.info("calc_node_start", q_id=answer_unit.get("q_id", "unknown"))
    
    # Convert dict to AnswerUnit model
    # Handle the case where steps might not be in the expected format
    if "steps" in answer_unit and isinstance(answer_unit["steps"], list):
        # Ensure steps are in the expected dictionary format
        steps = []
        for step in answer_unit["steps"]:
            if isinstance(step, dict):
                steps.append(step)
            # If steps are already in the correct format, keep them as is
        answer_unit["steps"] = steps
    
    answer_unit_model = AnswerUnit(**answer_unit)
    
    # Step 1: Extract student answer steps
    steps = answer_unit_model.steps
    
    # Step 2: Prepare prompt using the new prompt_utils module
    try:
        template_path = "prompts/calc.txt"
        student_answer = answer_unit_model.text
        # For now, we use the student answer as both problem and correct_answer since we don't have the correct answer
        # In a real implementation, you would get the correct answer from the problem store
        problem = answer_unit_model.stem
        correct_answer = answer_unit_model.correct_ans
        prompt = prepare_calc_prompt(template_path, problem, student_answer, correct_answer, rubric)
        
        # In a real implementation, you would call an LLM with this prompt
        # For now, we'll just log that we would use it
        logger.info("calc_prompt_prepared", prompt=prompt[:100] + "..." if len(prompt) > 100 else prompt)
        
        # Step 3: Call LLM with the prepared prompt using connection pooling
        try:
            # Use provided LLM client or get shared LLM client for connection pooling
            if llm is None:
                llm = get_llm_client()
            from langchain.schema import HumanMessage
            
            # Add retry logic for LLM calls
            max_retries = 3
            retry_count = 0
            while retry_count < max_retries:
                try:
                    # Use ainvoke method for async calls
                    # response = await llm.ainvoke([HumanMessage(content=prompt)])
                    response = await run_in_threadpool(llm.invoke, [HumanMessage(content=prompt)])
                    # Log the raw response for debugging
                    logger.info("llm_raw_response", content=response.content[:500] + "..." if len(response.content) > 500 else response.content)
                    
                    # Parse the JSON response
                    llm_response = parse_llm_json_response(response.content)
                    break  # Success, exit retry loop
                except Exception as e:
                    retry_count += 1
                    logger.warning(f"LLM call attempt {retry_count} failed: {str(e)}")
                    if retry_count >= max_retries:
                        raise  # Re-raise the exception if all retries failed
                    # Wait a bit before retrying
                    import time
                    time.sleep(2)  # Increased delay to reduce API load
            else:
                # This should not happen, but just in case
                raise Exception("LLM call failed after all retries")
            
            # Create step scores from LLM response
            step_scores = []
            if "steps" in llm_response and isinstance(llm_response["steps"], list):
                for step in llm_response["steps"]:
                    try:
                        if isinstance(step, dict):
                            step_score = StepScore(
                                step_no=step.get("step_no", len(step_scores) + 1),
                                desc=step.get("desc", step.get("comment", f"Step {step.get('step_no', len(step_scores) + 1)}")),
                                is_correct=step.get("is_correct", True) if step.get("is_correct") is not None else True,
                                score=step.get("score", 0.0)
                            )
                            step_scores.append(step_score)
                    except Exception as step_creation_error:
                        logger.warning("step_creation_failed", error=str(step_creation_error), step_data=step)
            
            # Calculate total score and confidence
            total_score = llm_response.get("score", sum(step.score for step in step_scores) if step_scores else 5.0)
            overall_confidence = llm_response.get("confidence", 0.8)
            comment = llm_response.get("comment", f"The calculation contains {len(step_scores)} steps.")
            response_max_score = llm_response.get("max_score", max_score)  # Use AI response max_score if available
            
            # Ensure scores are within valid ranges
            total_score = max(0.0, min(total_score, response_max_score))
            overall_confidence = max(0.0, min(overall_confidence, 1.0))
            
            # Create correction object with LLM results
            try:
                correction = Correction(
                    q_id=answer_unit_model.q_id,
                    type="计算题",
                    score=total_score,
                    max_score=response_max_score,  # Use the AI response max_score
                    confidence=overall_confidence,
                    comment=comment,
                    steps=step_scores
                )
            except Exception as correction_error:
                logger.error("correction_creation_failed", error=str(correction_error), 
                           q_id=answer_unit_model.q_id, type="计算题", score=total_score, 
                           max_score=max_score, confidence=overall_confidence,
                           comment=comment,
                           steps_count=len(step_scores))
                raise
            
            logger.info("calc_node_complete", q_id=answer_unit_model.q_id, score=correction.score, steps_count=len(correction.steps))
            return correction
        except Exception as e:
            # Fallback to rule-based approach if LLM call fails
            logger.warning("llm_call_failed", error=str(e))
            # Create a simple rule-based correction as fallback
            correction = Correction(
                q_id=answer_unit_model.q_id,
                type="计算题",
                score=5.0,  # Default score
                max_score=max_score,
                confidence=0.5,  # Default confidence
                comment="LLM call failed, using default scoring.",
                steps=[
                    StepScore(
                        step_no=1,
                        desc="LLM call failed, using default scoring.",
                        is_correct=True,
                        score=5.0
                    )
                ]
            )
            return correction
    except FileNotFoundError:
        logger.warning("calc_prompt_template_not_found", template_path=template_path)
        # Create a simple rule-based correction as fallback with a default prompt
        default_prompt = f"""
        You are a mathematics teacher who needs to grade a student's answer to a calculation problem.

        Problem:
        {answer_unit_model.stem}

        Student Answer:
        {answer_unit_model.text}

        Correct Answer:
        {answer_unit_model.correct_ans}

        Grading Rubric:
        {rubric}

        Please return the JSON result in the following format:
        {{
            "score": Score between 0-10,
            "max_score": 10,
            "confidence": Confidence between 0-1,
            "comment": "Feedback",
            "steps": [
                {{
                    "step_no": 1,
                    "desc": "Step description",
                    "is_correct": true/false,
                    "score": Score
                }}
            ]
        }}
        """
        
        # Try to call LLM with default prompt
        try:
            # Use provided LLM client or get shared LLM client for connection pooling
            if llm is None:
                llm = get_llm_client()
            from langchain.schema import HumanMessage
            
            # Add retry logic for LLM calls
            max_retries = 3
            retry_count = 0
            while retry_count < max_retries:
                try:
                    # response = await llm.ainvoke([HumanMessage(content=default_prompt)])
                    response = await run_in_threadpool(llm.invoke, [HumanMessage(content=default_prompt)])
                    llm_response = parse_llm_json_response(response.content)
                    break  # Success, exit retry loop
                except Exception as e:
                    retry_count += 1
                    logger.warning(f"Fallback LLM call attempt {retry_count} failed: {str(e)}")
                    if retry_count >= max_retries:
                        raise  # Re-raise the exception if all retries failed
                    # Wait a bit before retrying
                    import time
                    time.sleep(2)  # Increased delay to reduce API load
            else:
                # This should not happen, but just in case
                raise Exception("Fallback LLM call failed after all retries")
            
            # Create step scores from LLM response
            step_scores = []
            if "steps" in llm_response and isinstance(llm_response["steps"], list):
                for step in llm_response["steps"]:
                    try:
                        if isinstance(step, dict):
                            step_score = StepScore(
                                step_no=step.get("step_no", len(step_scores) + 1),
                                desc=step.get("desc", step.get("comment", f"Step {step.get('step_no', len(step_scores) + 1)}")),
                                is_correct=step.get("is_correct", True) if step.get("is_correct") is not None else True,
                                score=step.get("score", 0.0)
                            )
                            step_scores.append(step_score)
                    except Exception as step_creation_error:
                        logger.warning("step_creation_failed", error=str(step_creation_error), step_data=step)
            
            # Calculate total score and confidence
            total_score = llm_response.get("score", sum(step.score for step in step_scores) if step_scores else 5.0)
            overall_confidence = llm_response.get("confidence", 0.8)
            comment = llm_response.get("comment", f"The calculation contains {len(step_scores)} steps.")
            response_max_score = llm_response.get("max_score", max_score)  # Use AI response max_score if available
            
            # Ensure scores are within valid ranges
            total_score = max(0.0, min(total_score, response_max_score))
            overall_confidence = max(0.0, min(overall_confidence, 1.0))
            
            # Create correction object with LLM results
            correction = Correction(
                q_id=answer_unit_model.q_id,
                type="计算题",
                score=total_score,
                max_score=response_max_score,
                confidence=overall_confidence,
                comment=comment,
                steps=step_scores
            )
            return correction
        except Exception as fallback_error:
            logger.warning("fallback_llm_call_failed", error=str(fallback_error))
            # Create a simple rule-based correction as final fallback
            correction = Correction(
                q_id=answer_unit_model.q_id,
                type="计算题",
                score=5.0,  # Default score
                max_score=max_score,
                confidence=0.5,  # Default confidence
                comment="Template file not found. Using default scoring.",
                steps=[
                    StepScore(
                        step_no=1,
                        desc="Template file not found. Using default scoring.",
                        is_correct=True,
                        score=5.0
                    )
                ]
            )
            return correction
    except Exception as e:
        logger.warning("prompt_preparation_failed", error=str(e))
        # Create a simple rule-based correction as fallback
        correction = Correction(
            q_id=answer_unit_model.q_id,
            type="计算题",
            score=5.0,  # Default score
            max_score=max_score,
            confidence=0.5,  # Default confidence
            comment=f"Prompt preparation failed: {str(e)}",
            steps=[
                StepScore(
                    step_no=1,
                    desc=f"Prompt preparation failed: {str(e)}",
                    is_correct=True,
                    score=5.0
                )
            ]
        )
        return correction

def process_calc_from_files(input_file: str, rubric_file: str, output_file: str, max_score: float = 10.0):
    """
    Process a calculation question from input files and write results to output file.
    
    Args:
        input_file: Path to the JSON file containing the answer unit
        rubric_file: Path to the text file containing the grading rubric
        output_file: Path to the output JSON file for the correction result
        max_score: The maximum score for this question
    """
    # Read the answer unit from input file
    with open(input_file, 'r', encoding='utf-8') as f:
        answer_unit = json.load(f)
    
    # Read the rubric from rubric file
    with open(rubric_file, 'r', encoding='utf-8') as f:
        rubric = f.read().strip()
    
    # Process the calculation question
    correction = calc_node(answer_unit, rubric, max_score)
    
    # Log the correction result before writing
    logger.info("writing_correction_to_file", output_file=output_file, 
                score=correction.score, max_score=correction.max_score, 
                comment=correction.comment, steps_count=len(correction.steps))
    
    # Write the correction result to output file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(correction.model_dump_json(indent=2))
    
    logger.info("calc_processing_complete", input_file=input_file, output_file=output_file)
    return correction

if __name__ == "__main__":
    # Set up argument parser for file-based processing
    parser = argparse.ArgumentParser(description="Process calculation question from files")
    parser.add_argument("--input", required=True, help="Input JSON file containing answer unit")
    parser.add_argument("--rubric", required=True, help="Rubric text file")
    parser.add_argument("--output", required=True, help="Output JSON file for correction result")
    parser.add_argument("--max-score", type=float, default=10.0, help="Maximum score for this question")
    
    args = parser.parse_args()
    
    try:
        correction = process_calc_from_files(args.input, args.rubric, args.output, args.max_score)
        print(f"Calculation question processed successfully. Results written to {args.output}")
        print(f"Score: {correction.score}/{correction.max_score}")
        print(f"Comment: {correction.comment}")
        # Print steps for debugging
        print(f"Steps: {len(correction.steps)}")
        for step in correction.steps:
            print(f"  Step {step.step_no}: {step.desc} (Correct: {step.is_correct}, Score: {step.score})")
    except Exception as e:
        logger.error("calc_processing_failed", error=str(e), exc_info=True)
        print(f"Error processing calculation question: {e}")
        exit(1)